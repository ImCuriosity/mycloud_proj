import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
import dgl.nn as dglnn
import dgl.function as fn
from sklearn.metrics import roc_auc_score
from tqdm import tqdm

# ==========================================
# âš™ï¸ ì„¤ì • (Settings) - ì—…ê·¸ë ˆì´ë“œ
# ==========================================
PYG_GRAPH_PATH = "./outputs/graph/pet_graph_data.pt"
DGL_GRAPH_PATH = "./outputs/graph/pet_graph_dgl.bin"
MODEL_SAVE_PATH = "./outputs/graph/dgl_gnn_model_v2.pth"  # v2ë¡œ ë³€ê²½
EMBEDDING_SAVE_PATH = "./outputs/graph/dgl_node_embeddings_v2.pt" # v2ë¡œ ë³€ê²½

HIDDEN_DIMS = 128  # ğŸš€ [UP] 64 -> 128 (í‘œí˜„ë ¥ ì¦ê°€)
EPOCHS = 300       # MLPë¥¼ ì“°ë©´ ìˆ˜ë ´ì´ ë¹¨ë¼ì§ˆ ìˆ˜ ìˆì–´ 300ìœ¼ë¡œ ì¡°ì •
LR = 0.0005        # ğŸš€ [DOWN] ì •ë°€í•œ í•™ìŠµì„ ìœ„í•´ í•™ìŠµë¥  ì•½ê°„ ê°ì†Œ

# ==========================================
# ğŸ› ï¸ ë°ì´í„° ë¡œë“œ ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ê³¼ ë™ì¼)
# ==========================================
def convert_pyg_to_dgl(pyg_path):
    print("ğŸ”„ PyG ë°ì´í„° ë¡œë“œ ë° DGL ë³€í™˜ ì‹œì‘ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)...")
    try:
        pyg_data = torch.load(pyg_path, weights_only=False)
    except TypeError:
        pyg_data = torch.load(pyg_path)
    
    data_dict = {}
    num_nodes_dict = {ntype: pyg_data[ntype].num_nodes for ntype in pyg_data.node_types}

    for edge_type in pyg_data.edge_types:
        src_type, rel, dst_type = edge_type
        edge_index = pyg_data[edge_type].edge_index
        src = edge_index[0].cpu().numpy()
        dst = edge_index[1].cpu().numpy()
        data_dict[(src_type, rel, dst_type)] = (src, dst)

    g = dgl.heterograph(data_dict, num_nodes_dict=num_nodes_dict)
    return g

def get_dgl_graph(force_reload=False):
    if os.path.exists(DGL_GRAPH_PATH) and not force_reload:
        print(f"âœ… ìºì‹œëœ DGL ê·¸ë˜í”„ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {DGL_GRAPH_PATH}")
        g_list, _ = dgl.load_graphs(DGL_GRAPH_PATH)
        return g_list[0]
    else:
        if not os.path.exists(PYG_GRAPH_PATH):
            raise FileNotFoundError(f"âŒ ì›ë³¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {PYG_GRAPH_PATH}")
        g = convert_pyg_to_dgl(PYG_GRAPH_PATH)
        print(f"ğŸ’¾ DGL ê·¸ë˜í”„ë¥¼ ìºì‹±í•©ë‹ˆë‹¤: {DGL_GRAPH_PATH}")
        os.makedirs(os.path.dirname(DGL_GRAPH_PATH), exist_ok=True)
        dgl.save_graphs(DGL_GRAPH_PATH, [g])
        return g

# ==========================================
# ğŸ§  GNN ëª¨ë¸ ì •ì˜ (3-Layer HeteroSAGE)
# ==========================================
class HeteroSAGE(nn.Module):
    def __init__(self, g, in_feats, h_feats, out_feats):
        super().__init__()
        self.node_embeddings = nn.ModuleDict()
        for ntype in g.ntypes:
            self.node_embeddings[ntype] = nn.Embedding(g.num_nodes(ntype), in_feats)
        
        # ğŸš€ [UP] 3ì¸µ êµ¬ì¡°ë¡œ ë³€ê²½ (Deep GNN)
        self.layers = nn.ModuleList()
        
        # Layer 1
        self.layers.append(dglnn.HeteroGraphConv({
            etype: dglnn.SAGEConv(in_feats, h_feats, 'mean')
            for etype in g.etypes
        }, aggregate='sum'))
        
        # Layer 2
        self.layers.append(dglnn.HeteroGraphConv({
            etype: dglnn.SAGEConv(h_feats, h_feats, 'mean')
            for etype in g.etypes
        }, aggregate='sum'))
        
        # Layer 3 (Output)
        self.layers.append(dglnn.HeteroGraphConv({
            etype: dglnn.SAGEConv(h_feats, out_feats, 'mean')
            for etype in g.etypes
        }, aggregate='sum'))

        self.dropout = nn.Dropout(0.5) # ê³¼ì í•© ë°©ì§€

    def forward(self, g, x_dict=None):
        if x_dict is None:
            x_dict = {ntype: emb.weight for ntype, emb in self.node_embeddings.items()}
        
        h = x_dict
        
        # ë ˆì´ì–´ ë°˜ë³µ í†µê³¼ (Residual Connection í¬í•¨)
        for i, layer in enumerate(self.layers):
            h_new = layer(g, h)
            
            # ë…¸ë“œ ì†Œì‹¤ ë°©ì§€ (Residual)
            for ntype in h:
                if ntype not in h_new:
                    h_new[ntype] = h[ntype] # ì´ì „ ê°’ ìœ ì§€
                else:
                    # ì°¨ì›ì´ ê°™ì„ ë•Œë§Œ Residual ë”í•˜ê¸° (Skip Connection)
                    if h[ntype].shape == h_new[ntype].shape:
                         h_new[ntype] = h_new[ntype] + h[ntype]
            
            h = h_new
            
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ì•„ë‹ˆë©´ Activation & Dropout ì ìš©
            if i < len(self.layers) - 1:
                h = {k: self.dropout(F.relu(v)) for k, v in h.items()}
                
        return h

# ==========================================
# ğŸ§  MLP Predictor (í•µì‹¬ ì—…ê·¸ë ˆì´ë“œ)
# ==========================================
class MLPPredictor(nn.Module):
    """
    ë‹¨ìˆœ ë‚´ì (Dot Product) ëŒ€ì‹  ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²° í™•ë¥  ì˜ˆì¸¡
    (h_u, h_v) -> Linear -> ReLU -> Linear -> Score
    """
    def __init__(self, h_feats):
        super().__init__()
        # ì…ë ¥: ì†ŒìŠ¤ ë…¸ë“œ ë²¡í„° + íƒ€ê²Ÿ ë…¸ë“œ ë²¡í„° (concat)
        self.W1 = nn.Linear(h_feats * 2, h_feats)
        self.W2 = nn.Linear(h_feats, 1)

    def apply_edges(self, edges):
        # ì†ŒìŠ¤(src)ì™€ íƒ€ê²Ÿ(dst) ë²¡í„°ë¥¼ ì´ì–´ ë¶™ì„ (Concatenate)
        h = torch.cat([edges.src['x'], edges.dst['x']], dim=1)
        # MLP í†µê³¼
        score = self.W2(F.relu(self.W1(h))).squeeze(1)
        return {'score': score}

    def forward(self, edge_subgraph, x, target_etype):
        with edge_subgraph.local_scope():
            src_type, _, dst_type = target_etype
            
            # ë…¸ë“œ ë°ì´í„° í• ë‹¹
            if src_type in edge_subgraph.ntypes:
                edge_subgraph.nodes[src_type].data['x'] = x[src_type]
            if dst_type in edge_subgraph.ntypes:
                edge_subgraph.nodes[dst_type].data['x'] = x[dst_type]

            # íƒ€ê²Ÿ ì—£ì§€ì— ëŒ€í•´ apply_edges í•¨ìˆ˜ ì‹¤í–‰ (MLP ê³„ì‚°)
            edge_subgraph.apply_edges(self.apply_edges, etype=target_etype)
            return edge_subgraph.edges[target_etype].data['score']

# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
if __name__ == "__main__":
    g = get_dgl_graph()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âš¡ í•™ìŠµ ì¥ì¹˜: {device}")
    
    g = g.to(device)
    target_etype = ('company', 'files', 'trademark')
    
    # ì—£ì§€ í™•ì¸
    if target_etype not in g.canonical_etypes:
        target_etype = g.canonical_etypes[0]
    
    print(f"ğŸ¯ í•™ìŠµ íƒ€ê²Ÿ: {target_etype}")
    print(f"ğŸ§  ëª¨ë¸ êµ¬ì„±: 3-Layer HeteroSAGE + MLP Predictor (Hidden: {HIDDEN_DIMS})")

    # ëª¨ë¸ ì´ˆê¸°í™” (v2)
    model = HeteroSAGE(g, HIDDEN_DIMS, HIDDEN_DIMS, HIDDEN_DIMS).to(device)
    pred = MLPPredictor(HIDDEN_DIMS).to(device) # MLP Predictor ì‚¬ìš©
    
    # ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.Adam(
        list(model.parameters()) + list(pred.parameters()), 
        lr=LR
    )

    print("\nğŸš€ V2 ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    for epoch in range(1, EPOCHS + 1):
        model.train()
        pred.train() # Predictorë„ train ëª¨ë“œ
        
        # --- Negative Sampling ---
        src_node_count = g.num_nodes(target_etype[0])
        dst_node_count = g.num_nodes(target_etype[2])
        num_edges = g.num_edges(target_etype)
        
        neg_src = torch.randint(0, src_node_count, (num_edges,), device=device)
        neg_dst = torch.randint(0, dst_node_count, (num_edges,), device=device)
        
        neg_g = dgl.heterograph(
            {target_etype: (neg_src, neg_dst)},
            num_nodes_dict={nt: g.num_nodes(nt) for nt in g.ntypes}
        ).to(device)

        # --- Forward Pass ---
        h = model(g)
        
        pos_score = pred(neg_g, h, target_etype) # neg_g êµ¬ì¡° ì¬í™œìš© (ë³€ìˆ˜ëª… ì£¼ì˜: ì•„ë˜ì—ì„œ g ì‚¬ìš©)
        
        # DGL ë²„ê·¸ ë°©ì§€ë¥¼ ìœ„í•´ ì •í™•í•œ ê·¸ë˜í”„ ê°ì²´ ì „ë‹¬
        pos_score = pred(g, h, target_etype)
        neg_score = pred(neg_g, h, target_etype)
        
        # --- Loss ---
        scores = torch.cat([pos_score, neg_score])
        labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])
        loss = F.binary_cross_entropy_with_logits(scores, labels)
        
        # --- Backward ---
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # --- AUC ---
        if epoch % 10 == 0 or epoch == 1:
            with torch.no_grad():
                auc = roc_auc_score(labels.cpu().numpy(), scores.sigmoid().cpu().numpy())
                print(f"Epoch: {epoch:03d}/{EPOCHS}, Loss: {loss.item():.4f}, AUC: {auc:.4f}")

    print("\nğŸ’¾ V2 ê²°ê³¼ ì €ì¥ ì¤‘...")
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    
    model.eval()
    with torch.no_grad():
        final_h = model(g)
        final_h_cpu = {k: v.cpu() for k, v in final_h.items()}
        torch.save(final_h_cpu, EMBEDDING_SAVE_PATH)
        
    print(f"âœ… V2 í•™ìŠµ ì™„ë£Œ!\n - ëª¨ë¸: {MODEL_SAVE_PATH}\n - ì„ë² ë”©: {EMBEDDING_SAVE_PATH}")